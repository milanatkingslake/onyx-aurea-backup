# Introduction

The [Eng.AutomatedUnitTest](AutomatedUnitTest@aurea.com) team process source codes only, where the coverage of unit tests is calculated in a separate solution (than other teams). The team doesn't modify any existing production code.

For C# projects [Eng.AutomatedUnitTest](AutomatedUnitTest@aurea.com) usage C# TestGenerator.

# [C# Input Quality Bar] (http://bit.ly/2PHX5Ln)

- [Revision 0.4](http://bit.ly/2PHX5Ln) (08/28/2018)

# C# TestGenerator tool

This project has been created by the Automatic Unit test team. C# Generator tool has been improved by 40-50 CAs over many weeks. Provided tests are black-box and gray box testing. The tool provides business value in terms of coverage gain only.

# Project Structure and reporting

- AutomaticUnitTests
    - Contains all the bulk or automatic unit test projects. (Only additional files from AUT team). Located at root\SRC\AUT\*.*
- Projects (existing projects from the original solution)
- Readme
    - Readme.md

# AUT Build Instructions (root\SRC\AUT\MiddleTier.AUT.Tests.sln) Locally

  - Execute CompileGroups.ps1 (located on root/.jenkins/scripts)
  - Restore the Nuget Packages for the solution. Be Sure to have all the nuget feeds required to build root\AUT\Avolin.Onyx.AUT.Tests.sln
  - Using Visual Studio 2017, you should be able to build with no problems.

# Coverage Report Tool

AUT team usages Resharper dotCover code coverage tool (as a primary coverage report) or CI depending on the screenshot provided. Team finalizes the reporting based on the product. Based on the internal discussion we use the tool which is best suited to our needs and provide verification as a screenshot. There is always a difference in CI due to tooling difference. Every pull-request contains an initial and final screenshot. By running all tests coverage from the visual studio/or given screenshot CI it should yield the same number as the screen-shot. Plus, AUT team follows the independent coverage. For example, one team did 10K and we did 12K it doesn't mean it is going to sum up and show as 22K. Tests will be measured independently.

# Maintainability of Tests

Tests are not being maintained manually or by hand. Tests will be artificially generated by C# TestGenerator tool (v0.2.2) and added by the AUT team. Maintainability comes from the C# TestGenerator tool, as a team we don't modify the code by hand. If any test is failing due to future change, please get back to us, we will remove that test and generate a new ones using the tool.

For future maintenance, please get back to the developer or send email to "AutomatedUnitTest team <AutomatedUnitTest@aurea.com>".

# Minimum (.NET) Requirements for Tests

  All the dependencies above can be found at http://bit.ly/2AB2G2F
  
  1. .NET framework 4.6.1 (including sdk)
  2. .NET framework 4.6.2 (including sdk) 
  3. .NET framework 4.7 (including sdk)
  4. .NET framework 4.7.1 (including sdk)
  5. .NET framework 4.7.2 (including sdk)
  6. .NET core 1.0 (including sdk)
  7. .NET core 1.1 (including sdk)
  8. .NET core 2.0 (including sdk)
  9. .NET core 2.1 (including sdk)
  10. .NET core 2.2 (including sdk)

All the dependencies above can be found at https://www.microsoft.com/net/download/visual-studio-sdks

  **Please note that .NET runs side by side and existing projects will not be affected and do not need to be updated**

# How to check what versions of .NET Framework are installed

 https://docs.microsoft.com/en-us/dotnet/framework/migration-guide/how-to-determine-which-versions-are-installed#net_d

# Business Value and Coverage gain

We provide automated and meaningful tests within the context of the source code file. Consequently, if a project is integrated with a database or many others things and have larger domain scope. Then it would hard for the AI at this moment to get the whole context (in future probably it will have that capability). AUT tests isolates and single method or properties and tries to test it in a simple manner to gain coverage. Mostly focuses on getters/setters, constructors, fields. Consequently, our tests don't have much business logic, however, business value in terms of coverage gain.

Note : *Handcrafted Unit Test (HUT) team should provide appropriate business values with actual business logic.*

# Coverage Report Tool and verification

AUT team usages Visual studio code or CI (depending on the screenshot provided) coverage tool and all the reports are provided based on  that tool of which screenshot provided. There is always a difference in CI and VS due to tool difference. Every pull-request contains an initial and final screen-shot of the tool that has been used to capture the coverage gain. By running all tests coverage from visual studio or CI it should yield the same number as the screen-shot provided. Plus, team follow the independent coverage. For example, one team did 10K and we did 12K it doesn't mean it is going to sum up and show as 22K. Tests will be measured independently.

# Git Ignore

 Copy  [SampleGitIgnore] and paste it in the root and renamed to .gitignore (***Don't*** **PUSH gitignore it is for only our team** *Use tortoise git and put this as assumed unchanged*)

# Business Value

We provide automated and meaningful tests within the context of the source code file. Consequently, if a project is integrated with a database or many others things and have larger domain scope. Then it would hard for the AI at this moment to get the whole context (in future probably it will have that capability). AUT tests isolates and single method or properties and tries to test it in a simple manner to gain coverage. Mostly focuses on getters/setters, constructors, fields. Consequently, our tests don't have much business logic, however, business value in terms of coverage gain.

Note : *Handcrafted Unit Test (HUT) team should provide appropriate business values with actual business logic.*

## Why Automatic Unit Tests

Well, we can fake coverage gain by (http://bit.ly/2GiLlx3) or we can really do it automatically by our team and our tools.

# Code Quality

Our code quality is better than to existing products like EvoSuit or DiffBlue. Our codes tend to follow AAA: Arrange, Act and Assert format. Our codes have C# standard naming conventions, however, if the targeted source file doesn't have appropriate naming then it would fail. However, most of the part it follows standard naming conventions from MSDN.
  
## Generator Improvements

Our team always strive for coverage improvements, each week we have 5-10 CAs from crossover as a participant to improve the C#TestGenerator tools. As merging and reviewing the codes takes some, as a result, releases takes some time too. Our new version will have MS Fakes and MOQ, test quality will be far better. In the next iteration.

# Test Adapters
 
We use NUnit and Nunit 3.10.x Adapter for running and verifying the tests. Additionally, we use Autofixture 4.5 and Moq.

# Dependency Packages

Dependencies includes for our tests (AUT Team will added this to the project, external team doesn't require to add these):

 - .NET framework 4.7.1
 - AutoFixture 4.5.0
 - AutoFixture.AutoMoq 4.5.0
 - AutoFixture.NUnit3 4.5.0
 - Castle.Core 4.3.1
 - Fare 2.1.1
 - Moq 4.10.0
 - Nunit 3.10.1
 - NUnit3TestAdapter 3.12.0
 - Shouldly 3.0.0
 - System.Runtime.CompilerServices.Unsafe 4.5.1
 - System.Threading.Tasks.Extensions 4.5.1
 - System.ValueTuple 4.5.0

  <package id="AUT.ConfigureTestProjects" version="5.4.4" targetFramework="net462" />
  <package id="AUT.TestProjects.GenV3" version="1.4.0" targetFramework="net462" />
  <package id="AutoFixture" version="4.5.0" targetFramework="net462" />
  <package id="AutoFixture.AutoMoq" version="4.5.0" targetFramework="net462" />
  <package id="AutoFixture.NUnit3" version="4.5.0" targetFramework="net462" />
  <package id="Castle.Core" version="4.3.1" targetFramework="net462" />
  <package id="Fare" version="2.1.1" targetFramework="net462" />
  <package id="Moq" version="4.10.0" targetFramework="net462" />
  <package id="NUnit" version="3.10.1" targetFramework="net462" />
  <package id="NUnit3TestAdapter" version="3.12.0" targetFramework="net462" />
  <package id="Shouldly" version="3.0.0" targetFramework="net462" />
  <package id="System.Runtime.CompilerServices.Unsafe" version="4.5.2" targetFramework="net462" />
  <package id="System.Threading.Tasks.Extensions" version="4.5.1" targetFramework="net462" />
  <package id="System.ValueTuple" version="4.5.0" targetFramework="net462" />


Just to confirm that we can add these to our projects. So that we don't get blocked in the middle.

# Service Level Agreement (SLA)

Our team seeks a strict 24 hours SLA. Any pull-request created should be reviewed by 24 hours and if no issues then should be approved and merged by the SLA time period. In case, SLA is not honored, it goes through VP and to the upper management. As it is very strict and so far we follow this SLA with every product and team we work with.
  
# Coverage Calculation

We mostly follow [LOC](http://bit.ly/2sDGL3z) metrics.
Note: ***Sometimes there is possiblity of a negative percentage, due to adding new references.***

*For example, we are testing one class which is using another project as reference, however, we haven't added any tests to that project yet. In that case it will come as negative percentage for the time being. However, when we add tests to that reference project or complete whole project. It will have positive percentage gain when all are done.*

# Review Criteria

AUT tests are created using automated tool, thus, pull-request size is lot more larger (60-1200 files per pull-request) and there is no need to check generated codes line by line. For this reason, the PCA should verify build, no failing tests, coverage gain report using visual studio or by the screenshot provided. 
For starter, team should provide 50 tests (1-2 files) to PCA to review and understand how it is going to build in CI.

### Reject Cases : 
  - If there are double empty lines in the code, files are missing from solution which is causing build fails.
  - Coverage is NOT matching with the screenshot.
    - It needs to be huge (coverage gain is not matching over 500) difference from the provided screenshot and the exact tool.
    - Some tests sometimes not get calculated in the Jenkins or other CIs.
    - In PCA's machine or based on the screenshot of the tool. (different tooling will have different numbers due to different configurations).
  - Tests numbers are not matching in large numbers (over 300 tests).

### Acceptable cases with issues: 
  - Build failing in CI due to integration and it was passing in locally. It should be informed by comment to check the issue or investigate together.
  - Tests are failing in CI, however, passing locally (which can be verified by the provided screenshot that there is no failing tests from our end). It should be informed by comment to check the issue or investigate together.

# Working IC Contacts

|Name | Email |Skype |Slack|
|--|--|--|--|
|Kontantinos Papakonstantinou|konstantinos.p@aurea.com|papakost82 |konstantinos.papakonstantinou|

## Avolin Pivotal team contacts

SEM: 
Praveen Vedha <praveen.vedha@aurea.com>

PCA: 
Peter Sereda <peter.sereda@aurea.com>